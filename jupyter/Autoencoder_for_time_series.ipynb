{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# %load -s loadData ../scripts/dataUtilities.py \n",
    "def loadData(direc, dataset, perm = True, ratio_train = 0.8):\n",
    "    datadir = direc + '/' + dataset + '/' + dataset\n",
    "    data_train = np.genfromtxt(datadir + '_TRAIN.tsv', delimiter='\\t')\n",
    "    data_test_val = np.genfromtxt(datadir + '_TEST.tsv', delimiter='\\t')[:-1]\n",
    "    data = np.concatenate((data_train, data_test_val), axis=0)\n",
    "\n",
    "    N, D = data.shape\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    if perm:\n",
    "        ind = np.random.permutation(N)\n",
    "    else:\n",
    "        ind = range(0, N)\n",
    "    return data[ind[:ind_cut], 1:], data[ind[ind_cut:], 1:], data[ind[:ind_cut], 0], data[ind[ind_cut:], 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain, dataTest, labelsTrain, labelsTest = loadData('../data/UCRArchive_2018', 'ECG5000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()  # Put all configuration information into the dict\n",
    "config['num_layers'] = 2  # number of layers of stacked RNN's\n",
    "config['hidden_size'] = 90  # memory cells in a layer\n",
    "config['max_grad_norm'] = 5  # maximum gradient norm during training\n",
    "config['batch_size'] = batch_size = 64 # number of samples for iteration\n",
    "config['learning_rate'] = .005 # for exponential decay\n",
    "config['crd'] = 1  # Hyperparameter for future generalization\n",
    "config['num_l'] = 32 # number of units in the latent space\n",
    "\n",
    "plot_every = 100  # after _plot_every_ GD steps, there's console output\n",
    "max_iterations = 1000  # maximum number of iterations\n",
    "dropout = 0.8 # Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valuate sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain.shape[0]\n",
    "N = dataTrain.shape[0]\n",
    "Nval = dataTest.shape[0]\n",
    "D = dataTrain.shape[1]\n",
    "config['sl'] = sl = D  # sequence length\n",
    "print('We have %s observations with %s dimensions' % (N, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valuate class number and min class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labelsTrain))\n",
    "base = np.min(labelsTrain)  # Check if data is 0-based\n",
    "if base != 0:\n",
    "    labelsTrain -= base\n",
    "    labelsTest -= base\n",
    "\n",
    "print('We have %s classes, %s is the min class value' % (num_classes, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# %load -s plot_data ../scripts/plotUtilities.py \n",
    "def plot_data(data, classes, plot_row=10, save = False, name = 'tmp', adjust=True):\n",
    "    counts = dict(Counter(classes))\n",
    "    uniqueClasses = np.unique(classes)\n",
    "    num_classes = len(uniqueClasses)\n",
    "    f, axarr = plt.subplots(plot_row, num_classes)\n",
    "    for selectedClass in uniqueClasses:  # Loops over classes, plot as columns\n",
    "        selectedClass = int(selectedClass)\n",
    "        ind = np.where(classes == selectedClass)\n",
    "        ind_plot = np.random.choice(ind[0], size=plot_row)\n",
    "        for n in range(plot_row):  # Loops over rows\n",
    "            # Only shops axes for bottom row and left column\n",
    "            if n == 0:\n",
    "                axarr[n, selectedClass].set_title('Class %.0f (%.0f elements)' % (selectedClass + 1, counts[float(selectedClass)]))\n",
    "            if n < counts[float(selectedClass)]:\n",
    "                axarr[n, selectedClass].plot(data[ind_plot[n], :])\n",
    "\n",
    "                if not n == plot_row - 1:\n",
    "                    plt.setp([axarr[n, selectedClass].get_xticklabels()], visible=False)\n",
    "                if not selectedClass == 0:\n",
    "                    plt.setp([axarr[n, selectedClass].get_yticklabels()], visible=False)\n",
    "    \n",
    "    if adjust == True:\n",
    "        f.subplots_adjust(hspace=0)  # No horizontal space between subplots\n",
    "        f.subplots_adjust(wspace=0)  # No vertical space between subplots\n",
    "    plt.show()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(name, format='png', dpi=1000)\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data class plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "plot_data(dataTrain, labelsTrain)\n",
    "# plot_data(dataTrain, labelsTrain, 5, True, \"Prova.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.nn.rnn_cell import LSTMCell\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, config):\n",
    "        # Hyperparameters of the net\n",
    "        num_layers = config['num_layers']\n",
    "        hidden_size = config['hidden_size']\n",
    "        max_grad_norm = config['max_grad_norm']\n",
    "        batch_size = config['batch_size']\n",
    "        sl = config['sl']\n",
    "        crd = config['crd']\n",
    "        num_l = config['num_l']\n",
    "        learning_rate = config['learning_rate']\n",
    "        self.sl = sl\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Nodes for the input variables\n",
    "        self.x = tf.placeholder(\"float\", shape=[batch_size, sl], name='Input_data')\n",
    "        self.x_exp = tf.expand_dims(self.x, 1)\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            # Th encoder cell, multi-layered with dropout\n",
    "            cell_enc = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "            cell_enc = tf.contrib.rnn.DropoutWrapper(cell_enc, output_keep_prob=self.keep_prob)\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_enc = cell_enc.zero_state(batch_size, tf.float32)\n",
    "\n",
    "            # with tf.name_scope(\"Enc_2_lat\") as scope:\n",
    "            # layer for mean of z\n",
    "            W_mu = tf.get_variable('W_mu', [hidden_size, num_l])\n",
    "\n",
    "            outputs_enc, _ = tf.contrib.rnn.static_rnn(cell_enc,\n",
    "                                                       inputs=tf.unstack(self.x_exp, axis=2),\n",
    "                                                       initial_state=initial_state_enc)\n",
    "            cell_output = outputs_enc[-1]\n",
    "            b_mu = tf.get_variable('b_mu', [num_l])\n",
    "\n",
    "            # self.z_mu is the Tensor containing the hidden representations\n",
    "            # It can be used to do visualization, clustering or subsequent classification\n",
    "            self.z_mu = tf.nn.xw_plus_b(cell_output, W_mu, b_mu, name='z_mu')  # mu, mean, of latent space\n",
    "\n",
    "            # Train the point in latent space to have zero-mean and unit-variance on batch basis\n",
    "            lat_mean, lat_var = tf.nn.moments(self.z_mu, axes=[1])\n",
    "            self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean) + lat_var - tf.log(lat_var) - 1)\n",
    "\n",
    "        with tf.name_scope(\"Lat_2_dec\"):\n",
    "            # layer to generate initial state\n",
    "            W_state = tf.get_variable('W_state', [num_l, hidden_size])\n",
    "            b_state = tf.get_variable('b_state', [hidden_size])\n",
    "            z_state = tf.nn.xw_plus_b(self.z_mu, W_state, b_state, name='z_state')  # mu, mean, of latent space\n",
    "\n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            # The decoder, also multi-layered\n",
    "            cell_dec = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_dec = tuple([(z_state, z_state)] * num_layers)\n",
    "            dec_inputs = [tf.zeros([batch_size, 1])] * sl\n",
    "            # outputs_dec, _ = tf.nn.seq2seq.rnn_decoder(dec_inputs, initial_state_dec, cell_dec)\n",
    "            outputs_dec, _ = tf.contrib.rnn.static_rnn(cell_dec,\n",
    "                                                       inputs=dec_inputs,\n",
    "                                                       initial_state=initial_state_dec)\n",
    "        with tf.name_scope(\"Out_layer\"):\n",
    "            params_o = 2 * crd  # Number of coordinates + variances\n",
    "            W_o = tf.get_variable('W_o', [hidden_size, params_o])\n",
    "            b_o = tf.get_variable('b_o', [params_o])\n",
    "            outputs = tf.concat(outputs_dec, axis=0)  # tensor in [sl*batch_size,hidden_size]\n",
    "            h_out = tf.nn.xw_plus_b(outputs, W_o, b_o)\n",
    "            h_mu, h_sigma_log = tf.unstack(tf.reshape(h_out, [sl, batch_size, params_o]), axis=2)\n",
    "            h_sigma = tf.exp(h_sigma_log)\n",
    "            dist = tfp.distributions.Normal(h_mu, h_sigma)\n",
    "            px = dist.log_prob(tf.transpose(self.x))\n",
    "            loss_seq = -px\n",
    "            self.loss_seq = tf.reduce_mean(loss_seq)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            # Use learning rte decay\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            lr = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.1, staircase=False)\n",
    "\n",
    "            self.loss = self.loss_seq + self.loss_lat_batch\n",
    "\n",
    "            # Route the gradients so that we can plot them on Tensorboard\n",
    "            tvars = tf.trainable_variables()\n",
    "            # We clip the gradients to prevent explosion\n",
    "            grads = tf.gradients(self.loss, tvars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "            self.numel = tf.constant([[0]])\n",
    "\n",
    "            # And apply the gradients\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "            gradients = zip(grads, tvars)\n",
    "            self.train_step = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "            #      for gradient, variable in gradients:  #plot the gradient of each trainable variable\n",
    "            #        if isinstance(gradient, ops.IndexedSlices):\n",
    "            #          grad_values = gradient.values\n",
    "            #        else:\n",
    "            #          grad_values = gradient\n",
    "            #\n",
    "            #        self.numel +=tf.reduce_sum(tf.size(variable))\n",
    "            #        tf.summary.histogram(variable.name, variable)\n",
    "            #        tf.summary.histogram(variable.name + \"/gradients\", grad_values)\n",
    "            #        tf.summary.histogram(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "\n",
    "            self.numel = tf.constant([[0]])\n",
    "        tf.summary.tensor_summary('lat_state', self.z_mu)\n",
    "        # Define one op to call all summaries\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # and one op to initialize the variables\n",
    "        self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model = Model(config)\n",
    "sess = tf.Session()\n",
    "perf_collect = np.zeros((2, int(np.floor(max_iterations / plot_every))))\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "# Start of the train\n",
    "epochs = np.floor(batch_size * max_iterations / N)\n",
    "\n",
    "print('Train with approximately %d epochs' % epochs)\n",
    "\n",
    "sess.run(model.init_op)\n",
    "\n",
    "step = 0  # Step is a counter for filling the numpy array perf_collect\n",
    "for i in range(max_iterations):\n",
    "    batch_ind = np.random.choice(N, batch_size, replace=False)\n",
    "    result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n",
    "                      feed_dict={model.x: dataTrain[batch_ind], model.keep_prob: dropout})\n",
    "    \n",
    "    if i % plot_every == 0:\n",
    "        # Save train performances\n",
    "        perf_collect[0, step] = loss_train = result[0]\n",
    "        loss_train_seq, lost_train_lat = result[1], result[2]\n",
    "\n",
    "        # Calculate and save validation performance\n",
    "        batch_ind_val = np.random.choice(Nval, batch_size, replace=False)\n",
    "\n",
    "        result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.merged],\n",
    "                          feed_dict={model.x: dataTest[batch_ind_val], model.keep_prob: 1.0})\n",
    "        perf_collect[1, step] = loss_val = result[0]\n",
    "        loss_val_seq, lost_val_lat = result[1], result[2]\n",
    "        # and save to Tensorboard\n",
    "        summary_str = result[3]\n",
    "\n",
    "        print(\"At %6s / %6s train (%5.3f, %5.3f, %5.3f), val (%5.3f, %5.3f,%5.3f) in order (total, seq, lat)\" % (\n",
    "        i, max_iterations, loss_train, loss_train_seq, lost_train_lat, loss_val, loss_val_seq, lost_val_lat))\n",
    "        step += 1\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(\"logs/ecg5000.ckpt\"), step)\n",
    "\n",
    "print(\"model trained, saved in logs directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get latent vector from model run on dataTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, save_path=os.path.join(\"logs/ecg5000.ckpt-10\"))\n",
    "start = 0\n",
    "label = []  # The label to save to visualize the latent space\n",
    "z_run = []\n",
    "\n",
    "while start + batch_size < Nval:\n",
    "    run_ind = range(start, start + batch_size)\n",
    "    z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: dataTest[run_ind], model.keep_prob: 1.0})\n",
    "    z_run.extend(z_mu_fetch.tolist())\n",
    "    start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test set on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "plot_data(dataTest, labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based on latent vector of net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from itertools import groupby\n",
    "\n",
    "def getClustering(features, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    res = kmeans.fit_predict(features)\n",
    "    kmeans.labels_ += 1\n",
    "    res = res -1\n",
    "    return res\n",
    "res = getClustering(z_run,3)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test set on clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "plot_data(dataTest, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score, adjusted_mutual_info_score, homogeneity_score, completeness_score, v_measure_score, fowlkes_mallows_score\n",
    "\n",
    "def purity(y_true, y_pred):\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(cm, axis=0)) / np.sum(cm) \n",
    "\n",
    "def DTWDistance(s1,s2,w=None):\n",
    "        '''\n",
    "        Calculates dynamic time warping Euclidean distance between two\n",
    "        sequences. Option to enforce locality constraint for window w.\n",
    "        '''\n",
    "        DTW={}\n",
    "        if w:\n",
    "            w = max(w, abs(len(s1)-len(s2)))\n",
    "            for i in range(-1,len(s1)):\n",
    "                for j in range(-1,len(s2)):\n",
    "                    DTW[(i, j)] = float('inf')\n",
    "\n",
    "        else:\n",
    "            for i in range(len(s1)):\n",
    "                DTW[(i, -1)] = float('inf')\n",
    "            for i in range(len(s2)):\n",
    "                DTW[(-1, i)] = float('inf')\n",
    "        DTW[(-1, -1)] = 0\n",
    "        for i in range(len(s1)):\n",
    "            if w:\n",
    "                for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "                    dist= (s1[i]-s2[j])**2\n",
    "                    DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "            else:\n",
    "                for j in range(len(s2)):\n",
    "                    dist= (s1[i]-s2[j])**2\n",
    "                    DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "        print(\"Done\")\n",
    "        return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n",
    "    \n",
    "ground_labels =[ int(lt) + 1 for lt in labelsTest[0:960] ]\n",
    "\n",
    "# Dunn Index is heavy and good for small datasets. Not used\n",
    "\n",
    "# Misura la densità del clustering, ovvero quanto un sample è simile agli altri punto dello stesso cluster \n",
    "# e quanto bene dista dal cluster più vicino usando una metrica di similarità (euclidea, cosine, ecc).\n",
    "# Questo score è la media di tutti i silhouette score di ciascun sample\n",
    "# DTW is fine for TS but it takes too long\n",
    "ss = silhouette_score(dataTest[0:960], res, metric='euclidean')\n",
    "print(\"Silhouette score\", ss)\n",
    "\n",
    "# DB: Misura la separazione tra cluster, compiendo una media artimetica delle similarità tra coppie di cluster più simili\n",
    "# , usandodi una misura di similarità tra cluster ad hoc che mette a rapporto la somma dei diametri \n",
    "# dei cluster (media distanza euclidea intra-cluster) e la distanza euclidea tra i rispettivi centroidi.\n",
    "# Più tende a 0 meglio è. Fvorisce cluster densi e ben distanti \n",
    "# Più veloce di silhouette ma limitato alla distanza euclidea\n",
    "db = davies_bouldin_score(dataTest[0:960], res)\n",
    "print(\"Davies Bouldin\", db)\n",
    "\n",
    "# Righe le label e colonne i cluster\n",
    "cm = contingency_matrix(ground_labels, res)\n",
    "print(\"Contingency Matrix\")\n",
    "print(cm)\n",
    "\n",
    "# Media tra tutti i cluster del numero di sample della label più presente di ciascun cluster.\n",
    "# Da' una misura di quanto bene il clustering copre il labelling. Se è 1, il clustering ha coperto tutte le label\n",
    "# , anche ricorrendo ad un numero di cluster maggiore delle classi\n",
    "pur = purity(ground_labels, res)\n",
    "print(\"Purity: \", pur)\n",
    "\n",
    "# ARI: fix dell'RI, che mette a rapporto il numero di true (se due sample sono nello stesso cluster allora hanno la stessa label \n",
    "# + se due sample sono in cluster diversi allora hanno diversa label) sul numero totaale di coppie non ordinate di sample\n",
    "# va bene quando si vuole un clustering molto fedele al labelling del dataset. Valida per dataset i cui sample appartengono a classi ben distanti.\n",
    "# Immune al random labelling: https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py\n",
    "# Rule of thumb: Use ARI when the ground truth clustering has large equal sized clusters\n",
    "ars = adjusted_rand_score(ground_labels, res)\n",
    "print(\"Adjusted Rand Index: \", ars)\n",
    "\n",
    "# FMS: Media geometrica di precision e recall pairwise\n",
    "fms = fowlkes_mallows_score(ground_labels, res)\n",
    "print(\"Fowlkes-Mallows score: \", fms)\n",
    "\n",
    "# AMIS: fix del MIS, basata sull'entropia di Von Neuman, calcolata per le label e per i cluster\n",
    "# Immune al random labelling\n",
    "# Rule of thumb: Usa AMI when the ground truth clustering is unbalanced and there exist small clusters\n",
    "amis = adjusted_mutual_info_score(ground_labels, res, average_method='arithmetic')\n",
    "print(\"Adjusted Mutual Information: \", amis)\n",
    "\n",
    "# Media armonica di Homogeneity e Completeness. \n",
    "# Homogeneity: Quanto un cluster ha sample di una sola label\n",
    "# Completeness: Quanto i sample di una label stanno in un solo cluster\n",
    "# Entrambi basati sull'entropia di Von Neumann\n",
    "# Debole al random clustering con alto numero di cluster. Non buono con dataset piccoli e/o grande numero di cluster\n",
    "vm = v_measure_score(ground_labels, res)\n",
    "print(\"V-Measure: \", vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize latent vector on PCA and tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def plot_z_run(z_run, label, ):\n",
    "    f1, ax1 = plt.subplots(2, 1)\n",
    "\n",
    "    # First fit a PCA\n",
    "    PCA_model = TruncatedSVD(n_components=3).fit(z_run)\n",
    "    z_run_reduced = PCA_model.transform(z_run)\n",
    "    ax1[0].scatter(z_run_reduced[:, 0], z_run_reduced[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[0].set_title('PCA on z_run')\n",
    "\n",
    "    # THen fit a tSNE\n",
    "    tSNE_model = TSNE(verbose=2, perplexity=80, min_grad_norm=1E-12, n_iter=3000)\n",
    "    z_run_tsne = tSNE_model.fit_transform(z_run)\n",
    "    ax1[1].scatter(z_run_tsne[:, 0], z_run_tsne[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[1].set_title('tSNE on z_run')\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the latent space coordinates of the validation set\n",
    "start = 0\n",
    "label = []  # The label to save to visualize the latent space\n",
    "z_run = []\n",
    "\n",
    "while start + batch_size < Nval:\n",
    "    run_ind = range(start, start + batch_size)\n",
    "    z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: dataTest[run_ind], model.keep_prob: 1.0})\n",
    "    z_run.append(z_mu_fetch)\n",
    "    start += batch_size\n",
    "\n",
    "z_run = np.concatenate(z_run, axis=0)\n",
    "label = labelsTest[:start]\n",
    "\n",
    "plot_z_run(z_run, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
