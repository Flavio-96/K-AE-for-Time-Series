{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%pylab inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../data/UCRArchive_2018'\n",
    "checkpoints_directory = 'checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['ECG5000', 'ECG200', 'ChlorineConcentration', 'FordA', 'FordB', 'PhalangesOutlinesCorrect'\n",
    "                 , 'RefrigerationDevices', 'TwoLeadECG', 'TwoPatterns']\n",
    "dataset_name = 'ECG200'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Utility Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../scripts/dataUtilities.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as pp\n",
    "\n",
    "\n",
    "def load_data(direc, dataset, perm=True, ratio_train=0.8):\n",
    "    datadir = direc + '/' + dataset + '/' + dataset\n",
    "    data_train = np.genfromtxt(datadir + '_TRAIN.tsv', delimiter='\\t')\n",
    "    data_test_val = np.genfromtxt(datadir + '_TEST.tsv', delimiter='\\t')[:-1]\n",
    "    data = np.concatenate((data_train, data_test_val), axis=0)\n",
    "\n",
    "    N, D = data.shape\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    if perm:\n",
    "        ind = np.random.permutation(N)\n",
    "    else:\n",
    "        ind = range(0, N)\n",
    "    return data[ind[:ind_cut], 1:], data[ind[ind_cut:], 1:], data[ind[:ind_cut], 0], data[ind[ind_cut:], 0]\n",
    "\n",
    "\n",
    "def rebuild_data(data_train, data_test, labels_train, labels_test):\n",
    "    all_data = np.concatenate((data_train, data_test))\n",
    "    all_labels = np.concatenate((labels_train, labels_test)).reshape(-1, 1)\n",
    "    return np.concatenate((all_labels, all_data), axis=1)\n",
    "\n",
    "\n",
    "def remove_outlier(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    for col in df:\n",
    "        low_threshold = df[col].quantile(0.03)\n",
    "        high_threshold = df[col].quantile(0.97)\n",
    "        df.loc[df[col] < low_threshold, col] = low_threshold\n",
    "        df.loc[df[col] > high_threshold, col] = high_threshold\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def scale_data(dataset):\n",
    "    min_max_scaler = pp.MinMaxScaler()\n",
    "    dataset_scaled = min_max_scaler.fit_transform(dataset)\n",
    "    return dataset_scaled\n",
    "\n",
    "\n",
    "def plot_dataframe(dataset, title):\n",
    "    pd.DataFrame(dataset).plot(legend=False, title=title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = load_data(data_directory, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset shapes and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = data_train.shape[0]\n",
    "test_size = data_test.shape[0]\n",
    "seq_length = data_train.shape[1] # Time series sequence length\n",
    "print('Train set samples:', train_size)\n",
    "print('Test set samples:', test_size)\n",
    "print('Dimensions (columns):', seq_length)\n",
    "\n",
    "num_classes = len(np.unique(labels_train))\n",
    "base = int(np.min(labels_train))  # Check if data is 0-based\n",
    "if base != 0:\n",
    "    labels_train -= base\n",
    "    labels_test -= base\n",
    "\n",
    "print('There are', num_classes, 'classes,', base, 'is the min class value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "full_dataset = rebuild_data(data_train, data_test, labels_train, labels_test)\n",
    "full_dataset_df = pd.DataFrame(full_dataset)\n",
    "full_dataset_df.head()\n",
    "full_dataset_df.describe()\n",
    "full_dataset_df.isnull().sum().sum()\n",
    "full_dataset_df.groupby([0]).count()\n",
    "full_dataset_df.plot(legend=False, title=\"Full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "#plot_data(data_train, 'Before')\n",
    "data_train = remove_outlier(data_train)\n",
    "#data_train = scale_data(data_train)\n",
    "#plot_data(data_train, 'After')\n",
    "\n",
    "# Test data\n",
    "#plot_data(data_test, 'Before')\n",
    "data_test = remove_outlier(data_test)\n",
    "#data_test = scale_data(data_test)\n",
    "#plot_data(data_test, 'After')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample plotting script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../scripts/plotUtilities.py \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def plot_data(data, classes, plot_row=10, save=False, name='tmp', adjust=True):\n",
    "    counts = dict(Counter(classes))\n",
    "    uniqueClasses = np.unique(classes)\n",
    "    num_classes = len(uniqueClasses)\n",
    "    f, axarr = plt.subplots(plot_row, num_classes)\n",
    "    for selectedClass in uniqueClasses:  # Loops over classes, plot as columns\n",
    "        selectedClass = int(selectedClass)\n",
    "        ind = np.where(classes == selectedClass)\n",
    "        ind_plot = np.random.choice(ind[0], size=plot_row)\n",
    "        for n in range(plot_row):  # Loops over rows\n",
    "            # Only shops axes for bottom row and left column\n",
    "            if n == 0:\n",
    "                axarr[n, selectedClass].set_title(\n",
    "                    'Class %.0f (%.0f elements)' % (selectedClass + 1, counts[float(selectedClass)]))\n",
    "            if n < counts[float(selectedClass)]:\n",
    "                axarr[n, selectedClass].plot(data[ind_plot[n], :])\n",
    "\n",
    "                if not n == plot_row - 1:\n",
    "                    plt.setp([axarr[n, selectedClass].get_xticklabels()], visible=False)\n",
    "                if not selectedClass == 0:\n",
    "                    plt.setp([axarr[n, selectedClass].get_yticklabels()], visible=False)\n",
    "\n",
    "    if adjust == True:\n",
    "        f.subplots_adjust(hspace=0)  # No horizontal space between subplots\n",
    "        f.subplots_adjust(wspace=0)  # No vertical space between subplots\n",
    "    plt.show()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(name, format='png', dpi=1000)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sample plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "plot_data(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net description\n",
    "**dropout**: A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. This is called dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and improve generalization error in deep neural networks of all kinds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../scripts/autoencoder.py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTMCell\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, config):\n",
    "        # Hyperparameters of the net\n",
    "        num_layers = config['num_layers']\n",
    "        hidden_size = config['hidden_size']\n",
    "        max_grad_norm = config['max_grad_norm']\n",
    "        batch_size = config['batch_size']\n",
    "        seq_length = config['seq_length']\n",
    "        crd = config['crd']\n",
    "        num_l = config['num_l']\n",
    "        learning_rate = config['learning_rate']\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Nodes for the input variables\n",
    "        self.x = tf.placeholder(\"float\", shape=[batch_size, seq_length], name='Input_data')\n",
    "        self.x_exp = tf.expand_dims(self.x, 1)\n",
    "        self.keep_prob = 1 - dropout\n",
    "\n",
    "        with tf.variable_scope(\"Encoder\", reuse=tf.AUTO_REUSE):\n",
    "            # The encoder cell, multi-layered with dropout\n",
    "            # Number of LSTM = hidden layer size\n",
    "            cell_enc = tf.keras.layers.StackedRNNCells([\n",
    "                tf.keras.layers.LSTMCell(\n",
    "                    hidden_size,\n",
    "                    dropout=self.keep_prob\n",
    "                ) for _ in range(num_layers)\n",
    "            ])\n",
    "            # Initial state, tuple for all lstms stacked\n",
    "            # layer for mean of z\n",
    "            W_mu = tf.get_variable('W_mu', [hidden_size, num_l])\n",
    "\n",
    "            # Creates a recurrent neural network specified by RNNCell cell\n",
    "            # outputs is a length T list of outputs (one for each input), or a nested tuple of such elements.\n",
    "            # in our case one output for each time series in input\n",
    "            stacked_layer = tf.keras.layers.RNN(cell_enc, unroll=True)\n",
    "\n",
    "            cell_output = stacked_layer(self.x_exp)\n",
    "            b_mu = tf.get_variable('b_mu', [num_l])\n",
    "\n",
    "            # self.z_mu is the Tensor containing the hidden representations\n",
    "            # It can be used to do visualization, clustering or subsequent classification\n",
    "            # tf.nn.xw_plus_b computes matmul(x, weights) + biases.\n",
    "            self.z_mu = tf.nn.xw_plus_b(cell_output, W_mu, b_mu, name='z_mu')  # mu, mean, of latent space\n",
    "\n",
    "            # Calculate the mean and variance of the latent space\n",
    "            # The mean and variance are calculated by aggregating the contents of z_mu across axes\n",
    "            lat_mean, lat_var = tf.nn.moments(self.z_mu, axes=[1])\n",
    "\n",
    "            # Train the point in latent space to have zero-mean and unit-variance on batch basis\n",
    "            self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean) + lat_var - tf.log(lat_var) - 1)\n",
    "\n",
    "        with tf.name_scope(\"Lat_2_dec\"):\n",
    "            # layer to generate initial state\n",
    "            W_state = tf.get_variable('W_state', [num_l, hidden_size])\n",
    "            b_state = tf.get_variable('b_state', [hidden_size])\n",
    "            z_state = tf.nn.xw_plus_b(self.z_mu, W_state, b_state, name='z_state')  # mu, mean, of latent space\n",
    "\n",
    "        # Similar steps as encoder\n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            # The decoder, also multi-layered\n",
    "            cell_dec = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_dec = tuple([(z_state, z_state)] * num_layers)\n",
    "            dec_inputs = [tf.zeros([batch_size, 1])] * seq_length\n",
    "\n",
    "            outputs_dec, _ = tf.contrib.rnn.static_rnn(cell_dec,\n",
    "                                                       inputs=dec_inputs,\n",
    "                                                       initial_state=initial_state_dec)\n",
    "        with tf.name_scope(\"Out_layer\"):\n",
    "            params_o = 2 * crd  # Number of coordinates + variances\n",
    "            W_o = tf.get_variable('W_o', [hidden_size, params_o])\n",
    "            b_o = tf.get_variable('b_o', [params_o])\n",
    "            outputs = tf.concat(outputs_dec, axis=0)  # tensor in [seq_length*batch_size,hidden_size]\n",
    "            h_out = tf.nn.xw_plus_b(outputs, W_o, b_o)\n",
    "            h_mu, h_sigma_log = tf.unstack(tf.reshape(h_out, [seq_length, batch_size, params_o]), axis=2)\n",
    "            h_sigma = tf.exp(h_sigma_log)\n",
    "            dist = tfp.distributions.Normal(h_mu, h_sigma)\n",
    "            px = dist.log_prob(tf.transpose(self.x))\n",
    "            loss_seq = -px\n",
    "            self.loss_seq = tf.reduce_mean(loss_seq)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            # Use learning rate decay\n",
    "            # Useful use a learning rate schedule to reduce learning rate as the training progresses.\n",
    "            lr = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.1, staircase=False)\n",
    "\n",
    "            self.loss = self.loss_seq + self.loss_lat_batch\n",
    "\n",
    "            # Route the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            # We clip the gradients to prevent explosion\n",
    "            grads = tf.gradients(self.loss, tvars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "            self.numel = tf.constant([[0]])\n",
    "\n",
    "            # And apply the gradients\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "            gradients = zip(grads, tvars)\n",
    "            self.train_step = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "\n",
    "            self.numel = tf.constant([[0]])\n",
    "\n",
    "        tf.summary.tensor_summary('lat_state', self.z_mu)\n",
    "        # Define one op to call all summaries\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # Returns an Op that initializes global variables.\n",
    "        self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_configs = {\n",
    "    'ECG5000': {\n",
    "        'num_layers': 2,  # number of layers of stacked RNN's\n",
    "        'hidden_size': 32,  # memory cells in a layer\n",
    "        'max_grad_norm': 5,  # maximum gradient norm during training\n",
    "        'batch_size': 64, # number of samples for iteration\n",
    "        'learning_rate': .005, # for exponential decay\n",
    "        'crd': 1,  # Hyperparameter for future generalization\n",
    "        'num_l': 32, # number of units in the latent space\n",
    "        'max_iterations': 1500\n",
    "    },\n",
    "    'ECG200': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 16,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 16,\n",
    "        'max_iterations': 2000\n",
    "    },\n",
    "    'ChlorineConcentration': {\n",
    "        'num_layers': 2,  # number of layers of stacked RNN's\n",
    "        'hidden_size': 48,  # memory cells in a layer\n",
    "        'max_grad_norm': 5,  # maximum gradient norm during training\n",
    "        'batch_size': 32, # number of samples for iteration\n",
    "        'learning_rate': .005, # for exponential decay\n",
    "        'crd': 1,  # Hyperparameter for future generalization\n",
    "        'num_l': 24, # number of units in the latent space\n",
    "        'max_iterations': 2000\n",
    "    },\n",
    "    'FordA': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32,\n",
    "        'max_iterations': 1000\n",
    "    },\n",
    "    'FordB': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32,\n",
    "        'max_iterations': 1000\n",
    "    },\n",
    "    'PhalangesOutlinesCorrect': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32,\n",
    "        'max_iterations': 1000\n",
    "    },\n",
    "    'RefrigerationDevices': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32,\n",
    "        'max_iterations': 1000\n",
    "    },\n",
    "    'TwoLeadECG': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': .001,\n",
    "        'crd': 1,\n",
    "        'num_l': 32,\n",
    "        'max_iterations': 1000\n",
    "    },\n",
    "    'TwoPatterns': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32,\n",
    "        'max_iterations': 1000\n",
    "    }\n",
    "}\n",
    "plot_every = 50  # after _plot_every_ GD steps, there's console output\n",
    "dropout = 0.8 # Dropout rate\n",
    "config = dataset_configs[dataset_name]\n",
    "config['seq_length'] = seq_length;  # Time series sequence length\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "os.makedirs(checkpoints_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(config)\n",
    "sess = tf.Session()\n",
    "\n",
    "max_iterations = config['max_iterations']  # maximum number of iterations\n",
    "perf_collect = np.zeros((2, int(np.ceil(max_iterations / plot_every)+1)))\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "# Number of samples according to batch_size\n",
    "actual_test_size = (test_size // batch_size) * batch_size\n",
    "actual_data_test = data_test[0:actual_test_size]\n",
    "actual_labels_test = labels_test[0:actual_test_size]\n",
    "\n",
    "# Start of the train\n",
    "epochs = np.floor(batch_size * max_iterations / train_size)\n",
    "\n",
    "print('Train with approximately %d epochs' % epochs)\n",
    "\n",
    "sess.run(model.init_op)\n",
    "\n",
    "step = 0  # Step is a counter for filling the numpy array perf_collect\n",
    "for i in range(max_iterations):\n",
    "    batch_ind = np.random.choice(train_size, batch_size, replace=False)\n",
    "    #current = (i * batch_size) % actual_train_size\n",
    "    #batch_ind = np.arange(current, current + batch_size)\n",
    "    result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n",
    "                      feed_dict={model.x: data_train[batch_ind]})\n",
    "    \n",
    "    if (i == 0) or (((i+1) % plot_every) == 0):\n",
    "        # Save train performances\n",
    "        perf_collect[0, step] = loss_train = result[0]\n",
    "        loss_train_seq, lost_train_lat = result[1], result[2]\n",
    "\n",
    "        # Calculate and save validation performance\n",
    "        batch_ind_val = np.random.choice(test_size, batch_size, replace=False)\n",
    "\n",
    "        result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.merged],\n",
    "                          feed_dict={model.x: data_test[batch_ind_val]})\n",
    "        perf_collect[1, step] = loss_val = result[0]\n",
    "        loss_val_seq, lost_val_lat = result[1], result[2]\n",
    "        # and save to Tensorboard\n",
    "        summary_str = result[3]\n",
    "\n",
    "        print(\"At %6s / %6s train (%5.3f, %5.3f, %5.3f), val (%5.3f, %5.3f,%5.3f) in order (total, seq, lat)\" % (\n",
    "        i+1, max_iterations, loss_train, loss_train_seq, lost_train_lat, loss_val, loss_val_seq, lost_val_lat))\n",
    "        step += 1\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(checkpoints_directory, dataset_name))\n",
    "\n",
    "print(\"model trained, saved in logs directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract latent vector of test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, save_path=os.path.join(checkpoints_directory, dataset_name))\n",
    "\n",
    "# Extract the latent space coordinates of the validation set\n",
    "\n",
    "start = 0\n",
    "label = []  # The label to save to visualize the latent space\n",
    "z_run_clust = [] # Latent space for clustering\n",
    "z_run_emb = [] # Latent space for tSNE\n",
    "\n",
    "while start + batch_size <= actual_test_size:\n",
    "    run_ind = range(start, start + batch_size)\n",
    "    z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: actual_data_test[run_ind]})\n",
    "    z_run_clust.extend(z_mu_fetch.tolist())\n",
    "    z_run_emb.append(z_mu_fetch)\n",
    "\n",
    "    start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize latent vector on PCA and tSNE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "def plot_z_run(z_run_emb, label, ):\n",
    "    f1, ax1 = plt.subplots(2, 1)\n",
    "\n",
    "    # First fit a PCA\n",
    "    PCA_model = TruncatedSVD(n_components=3).fit(z_run_emb)\n",
    "    z_run_reduced = PCA_model.transform(z_run_emb)\n",
    "    ax1[0].scatter(z_run_reduced[:, 0], z_run_reduced[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[0].set_title('PCA on z_run')\n",
    "\n",
    "    # Then fit a tSNE\n",
    "    tSNE_model = TSNE(verbose=2, perplexity=80, min_grad_norm=1E-12, n_iter=3000)\n",
    "    z_run_tsne = tSNE_model.fit_transform(z_run_emb)\n",
    "    ax1[1].scatter(z_run_tsne[:, 0], z_run_tsne[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[1].set_title('tSNE on z_run')\n",
    "\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "z_run_emb = np.concatenate(z_run_emb, axis=0)\n",
    "label = labels_test[:start]\n",
    "plot_z_run(z_run_emb, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "plot_data(actual_data_test, actual_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering on test latent vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../scripts/metrics.py\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score \\\n",
    "    , adjusted_mutual_info_score, v_measure_score, fowlkes_mallows_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Dunn Index is heavy and good for small datasets. TODO\n",
    "\n",
    "# Misura la densità del clustering, ovvero quanto un sample è simile agli altri punto dello stesso cluster\n",
    "# e quanto bene dista dal cluster più vicino usando una metrica di similarità (euclidea, cosine, ecc).\n",
    "# Questo score è la media di tutti i silhouette score di ciascun sample\n",
    "# DTW is fine for TS but it takes too long\n",
    "def silhouette(dataset, clustering):\n",
    "    return silhouette_score(dataset, clustering, metric='euclidean')\n",
    "\n",
    "\n",
    "# DB: Misura la separazione tra cluster, compiendo una media artimetica delle similarità tra coppie di cluster più simili\n",
    "# , usandodi una misura di similarità tra cluster ad hoc che mette a rapporto la somma dei diametri\n",
    "# dei cluster (media distanza euclidea intra-cluster) e la distanza euclidea tra i rispettivi centroidi.\n",
    "# Più tende a 0 meglio è. Fvorisce cluster densi e ben distanti\n",
    "# Più veloce di silhouette ma limitato alla distanza euclidea\n",
    "def db(dataset, clustering):\n",
    "    return davies_bouldin_score(dataset, clustering)\n",
    "\n",
    "\n",
    "# Righe le label e colonne i cluster\n",
    "def cm(y_true, y_pred):\n",
    "    return contingency_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Media tra tutti i cluster del numero di sample della label più presente di ciascun cluster.\n",
    "# Da' una misura di quanto bene il clustering copre il labelling. Se è 1, il clustering ha coperto tutte le label\n",
    "# , anche ricorrendo ad un numero di cluster maggiore delle classi\n",
    "def purity(y_true, y_pred):\n",
    "    cont_matrix = cm(y_true, y_pred)\n",
    "    return np.sum(np.amax(cont_matrix, axis=0)) / np.sum(cont_matrix)\n",
    "\n",
    "\n",
    "def rel_purity(y_true, y_pred):\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    labels_sum = np.sum(cm, axis=1)\n",
    "    rm = np.zeros(cm.shape)\n",
    "    for j in range(cm.shape[1]):\n",
    "        for i in range(cm.shape[0]):\n",
    "            rm[i][j] = cm[i][j] / labels_sum[i]\n",
    "    # print(\"Relative Contingency Matrix\")\n",
    "    # print(rm)\n",
    "    # print(np.max(rm, axis=0))\n",
    "\n",
    "    max_indexes = np.argmax(rm, axis=0)\n",
    "    # print(max_indexes)\n",
    "    sum = 0\n",
    "    for j in range(rm.shape[1]):\n",
    "        sum += cm[max_indexes[j]][j]\n",
    "    return sum / np.sum(cm)\n",
    "\n",
    "\n",
    "# ARI: fix dell'RI, che mette a rapporto il numero di true (se due sample sono nello stesso cluster allora hanno la stessa label\n",
    "# + se due sample sono in cluster diversi allora hanno diversa label) sul numero totaale di coppie non ordinate di sample\n",
    "# va bene quando si vuole un clustering molto fedele al labelling del dataset. Valida per dataset i cui sample appartengono a classi ben distanti.\n",
    "# Immune al random labelling: https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py\n",
    "# Rule of thumb: Use ARI when the ground truth clustering has large equal sized clusters\n",
    "def ari(y_true, y_pred):\n",
    "    return adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# FMS: Media geometrica di precision e recall pairwise\n",
    "def fmi(y_true, y_pred):\n",
    "    return fowlkes_mallows_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# AMIS: fix del MIS, basata sull'entropia di Von Neuman, calcolata per le label e per i cluster\n",
    "# Immune al random labelling\n",
    "# Rule of thumb: Usa AMI when the ground truth clustering is unbalanced and there exist small clusters\n",
    "def ami(y_true, y_pred):\n",
    "    return adjusted_mutual_info_score(y_true, y_pred, average_method='arithmetic')\n",
    "\n",
    "\n",
    "# Media armonica di Homogeneity e Completeness.\n",
    "# Homogeneity: Quanto un cluster ha sample di una sola label\n",
    "# Completeness: Quanto i sample di una label stanno in un solo cluster\n",
    "# Entrambi basati sull'entropia di Von Neumann\n",
    "# Debole al random clustering con alto numero di cluster. Non buono con dataset piccoli e/o grande numero di cluster\n",
    "def vm(y_true, y_pred):\n",
    "    return v_measure_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Clustering Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../scripts/clustering.py\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def getClustering(features, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=100)\n",
    "    clustering = kmeans.fit_predict(features)\n",
    "    kmeans.labels_ += 1\n",
    "    clustering = clustering -1\n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering, plotting and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_labels =[ int(lt) + 1 for lt in actual_labels_test ]\n",
    "if num_classes <= 2:\n",
    "    num_clusters = [2, 3, 4, 5, 6]\n",
    "else:\n",
    "    num_clusters = [num_classes - 2, num_classes - 1, num_classes, num_classes + 1, num_classes + 2]\n",
    "\n",
    "metric_df = pd.DataFrame()\n",
    "for nc in num_clusters:\n",
    "    print('Clustering: ', nc, '- Means')\n",
    "    clustering = getClustering(z_run_clust, nc)\n",
    "    pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "    plot_data(actual_data_test, clustering)\n",
    "    \n",
    "    metric_row = {'Silhouette': silhouette(actual_data_test, clustering),\n",
    "                  'DB': db(actual_data_test, clustering),\n",
    "                  'Purity': purity(ground_labels, clustering),\n",
    "                  'Rel. Purity': rel_purity(ground_labels, clustering),\n",
    "                  'ARI': ari(ground_labels, clustering),\n",
    "                  'FMI': fmi(ground_labels, clustering),\n",
    "                  'AMI': ami(ground_labels, clustering),\n",
    "                  'VM': vm(ground_labels, clustering)\n",
    "                 }\n",
    "    metric_row_df = pd.DataFrame(metric_row, index=[str(nc)])\n",
    "    metric_df = metric_df.append(metric_row_df)\n",
    "    print('Contingecy matrix\\n', cm(ground_labels, clustering))\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "metric_df.index.name = '#cluster'\n",
    "csv_filename = '../export/' + dataset_name + '_metrics.csv'\n",
    "metric_df.to_csv(path_or_buf=csv_filename)\n",
    "metric_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
