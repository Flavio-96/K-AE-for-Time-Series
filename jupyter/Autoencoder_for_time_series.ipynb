{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libreries for build and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s loadData ../scripts/dataUtilities.py \n",
    "def loadData(direc, dataset, perm = True, ratio_train = 0.8):\n",
    "    datadir = direc + '/' + dataset + '/' + dataset\n",
    "    data_train = np.genfromtxt(datadir + '_TRAIN.tsv', delimiter='\\t')\n",
    "    data_test_val = np.genfromtxt(datadir + '_TEST.tsv', delimiter='\\t')[:-1]\n",
    "    data = np.concatenate((data_train, data_test_val), axis=0)\n",
    "\n",
    "    N, D = data.shape\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    if perm:\n",
    "        ind = np.random.permutation(N)\n",
    "    else:\n",
    "        ind = range(0, N)\n",
    "    return data[ind[:ind_cut], 1:], data[ind[ind_cut:], 1:], data[ind[:ind_cut], 0], data[ind[ind_cut:], 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain, dataTest, labelsTrain, labelsTest = loadData('../data/UCRArchive_2018', 'ECG5000')\n",
    "labelsTrain[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()  # Put all configuration information into the dict\n",
    "config['num_layers'] = 2  # number of layers of stacked RNN's\n",
    "config['hidden_size'] = 90  # memory cells in a layer\n",
    "config['max_grad_norm'] = 5  # maximum gradient norm during training\n",
    "config['batch_size'] = batch_size = 64 # number of samples for iteration\n",
    "config['learning_rate'] = .005 # for exponential decay\n",
    "config['crd'] = 1  # Hyperparameter for future generalization\n",
    "config['num_l'] = 20 # number of units in the latent space\n",
    "\n",
    "plot_every = 100  # after _plot_every_ GD steps, there's console output\n",
    "max_iterations = 1000  # maximum number of iterations\n",
    "dropout = 0.8 # Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuate sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain.shape[0]\n",
    "N = dataTrain.shape[0]\n",
    "Nval = dataTest.shape[0]\n",
    "D = dataTrain.shape[1]\n",
    "config['sl'] = sl = D  # sequence length\n",
    "print('We have %s observations with %s dimensions' % (N, D))\n",
    "\n",
    "Nval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuate class number and min class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labelsTrain))\n",
    "base = np.min(labelsTrain)  # Check if data is 0-based\n",
    "if base != 0:\n",
    "    labelsTrain -= base\n",
    "    labelsTest -= base\n",
    "\n",
    "print('We have %s classes, %s is the min class value' % (num_classes, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s plot_data ../scripts/plotUtilities.py \n",
    "def plot_data(X_train, y_train, plot_row=5, save = False, name = 'tmp.eps'):\n",
    "    counts = dict(Counter(y_train))\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    f, axarr = plt.subplots(plot_row, num_classes)\n",
    "    for c in np.unique(y_train):  # Loops over classes, plot as columns\n",
    "        c = int(c)\n",
    "        ind = np.where(y_train == c)\n",
    "        ind_plot = np.random.choice(ind[0], size=plot_row)\n",
    "        for n in range(plot_row):  # Loops over rows\n",
    "            axarr[n, c].plot(X_train[ind_plot[n], :])\n",
    "            # Only shops axes for bottom row and left column\n",
    "            if n == 0:\n",
    "                axarr[n, c].set_title('Class %.0f (%.0f)' % (c, counts[float(c)]))\n",
    "            if not n == plot_row - 1:\n",
    "                plt.setp([axarr[n, c].get_xticklabels()], visible=False)\n",
    "            if not c == 0:\n",
    "                plt.setp([axarr[n, c].get_yticklabels()], visible=False)\n",
    "    f.subplots_adjust(hspace=0)  # No horizontal space between subplots\n",
    "    f.subplots_adjust(wspace=0)  # No vertical space between subplots\n",
    "    plt.show()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(name, format='eps', dpi=1000)\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "plot_data(dataTrain, labelsTrain)\n",
    "# plot_data(dataTrain, labelsTrain, 5, True, \"Prova.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, config):\n",
    "        # Hyperparameters of the net\n",
    "        num_layers = config['num_layers']\n",
    "        hidden_size = config['hidden_size']\n",
    "        max_grad_norm = config['max_grad_norm']\n",
    "        batch_size = config['batch_size']\n",
    "        sl = config['sl']\n",
    "        crd = config['crd']\n",
    "        num_l = config['num_l']\n",
    "        learning_rate = config['learning_rate']\n",
    "        self.sl = sl\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Nodes for the input variables\n",
    "        self.x = tf.placeholder(\"float\", shape=[batch_size, sl], name='Input_data')\n",
    "        self.x_exp = tf.expand_dims(self.x, 1)\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            # Th encoder cell, multi-layered with dropout\n",
    "            cell_enc = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "            cell_enc = tf.contrib.rnn.DropoutWrapper(cell_enc, output_keep_prob=self.keep_prob)\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_enc = cell_enc.zero_state(batch_size, tf.float32)\n",
    "\n",
    "            # with tf.name_scope(\"Enc_2_lat\") as scope:\n",
    "            # layer for mean of z\n",
    "            W_mu = tf.get_variable('W_mu', [hidden_size, num_l])\n",
    "\n",
    "            outputs_enc, _ = tf.contrib.rnn.static_rnn(cell_enc,\n",
    "                                                       inputs=tf.unstack(self.x_exp, axis=2),\n",
    "                                                       initial_state=initial_state_enc)\n",
    "            cell_output = outputs_enc[-1]\n",
    "            b_mu = tf.get_variable('b_mu', [num_l])\n",
    "\n",
    "            # For all intents and purposes, self.z_mu is the Tensor containing the hidden representations\n",
    "            # I got many questions over email about this. If you want to do visualization, clustering or subsequent\n",
    "            #   classification, then use this z_mu\n",
    "            self.z_mu = tf.nn.xw_plus_b(cell_output, W_mu, b_mu, name='z_mu')  # mu, mean, of latent space\n",
    "\n",
    "            # Train the point in latent space to have zero-mean and unit-variance on batch basis\n",
    "            lat_mean, lat_var = tf.nn.moments(self.z_mu, axes=[1])\n",
    "            self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean) + lat_var - tf.log(lat_var) - 1)\n",
    "\n",
    "        with tf.name_scope(\"Lat_2_dec\"):\n",
    "            # layer to generate initial state\n",
    "            W_state = tf.get_variable('W_state', [num_l, hidden_size])\n",
    "            b_state = tf.get_variable('b_state', [hidden_size])\n",
    "            z_state = tf.nn.xw_plus_b(self.z_mu, W_state, b_state, name='z_state')  # mu, mean, of latent space\n",
    "\n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            # The decoder, also multi-layered\n",
    "            cell_dec = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_dec = tuple([(z_state, z_state)] * num_layers)\n",
    "            dec_inputs = [tf.zeros([batch_size, 1])] * sl\n",
    "            # outputs_dec, _ = tf.nn.seq2seq.rnn_decoder(dec_inputs, initial_state_dec, cell_dec)\n",
    "            outputs_dec, _ = tf.contrib.rnn.static_rnn(cell_dec,\n",
    "                                                       inputs=dec_inputs,\n",
    "                                                       initial_state=initial_state_dec)\n",
    "        with tf.name_scope(\"Out_layer\"):\n",
    "            params_o = 2 * crd  # Number of coordinates + variances\n",
    "            W_o = tf.get_variable('W_o', [hidden_size, params_o])\n",
    "            b_o = tf.get_variable('b_o', [params_o])\n",
    "            outputs = tf.concat(outputs_dec, axis=0)  # tensor in [sl*batch_size,hidden_size]\n",
    "            h_out = tf.nn.xw_plus_b(outputs, W_o, b_o)\n",
    "            h_mu, h_sigma_log = tf.unstack(tf.reshape(h_out, [sl, batch_size, params_o]), axis=2)\n",
    "            h_sigma = tf.exp(h_sigma_log)\n",
    "            dist = tf.contrib.distributions.Normal(h_mu, h_sigma)\n",
    "            px = dist.log_prob(tf.transpose(self.x))\n",
    "            loss_seq = -px\n",
    "            self.loss_seq = tf.reduce_mean(loss_seq)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            # Use learning rte decay\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            lr = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.1, staircase=False)\n",
    "\n",
    "            self.loss = self.loss_seq + self.loss_lat_batch\n",
    "\n",
    "            # Route the gradients so that we can plot them on Tensorboard\n",
    "            tvars = tf.trainable_variables()\n",
    "            # We clip the gradients to prevent explosion\n",
    "            grads = tf.gradients(self.loss, tvars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "            self.numel = tf.constant([[0]])\n",
    "\n",
    "            # And apply the gradients\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "            gradients = zip(grads, tvars)\n",
    "            self.train_step = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "            #      for gradient, variable in gradients:  #plot the gradient of each trainable variable\n",
    "            #        if isinstance(gradient, ops.IndexedSlices):\n",
    "            #          grad_values = gradient.values\n",
    "            #        else:\n",
    "            #          grad_values = gradient\n",
    "            #\n",
    "            #        self.numel +=tf.reduce_sum(tf.size(variable))\n",
    "            #        tf.summary.histogram(variable.name, variable)\n",
    "            #        tf.summary.histogram(variable.name + \"/gradients\", grad_values)\n",
    "            #        tf.summary.histogram(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "\n",
    "            self.numel = tf.constant([[0]])\n",
    "        tf.summary.tensor_summary('lat_state', self.z_mu)\n",
    "        # Define one op to call all summaries\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # and one op to initialize the variables\n",
    "        self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_iterations):\n",
    "    batch_ind = np.random.choice(N, batch_size, replace=False)\n",
    "    result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n",
    "                      feed_dict={model.x: dataTrain[batch_ind], model.keep_prob: dropout})\n",
    "    \n",
    "batch_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config)\n",
    "sess = tf.Session()\n",
    "perf_collect = np.zeros((2, int(np.floor(max_iterations / plot_every))))\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "# Start of the train\n",
    "epochs = np.floor(batch_size * max_iterations / N)\n",
    "\n",
    "print('Train with approximately %d epochs' % epochs)\n",
    "\n",
    "sess.run(model.init_op)\n",
    "\n",
    "step = 0  # Step is a counter for filling the numpy array perf_collect\n",
    "for i in range(max_iterations):\n",
    "    batch_ind = np.random.choice(N, batch_size, replace=False)\n",
    "    result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n",
    "                      feed_dict={model.x: dataTrain[batch_ind], model.keep_prob: dropout})\n",
    "    \n",
    "    if i % plot_every == 0:\n",
    "        # Save train performances\n",
    "        perf_collect[0, step] = loss_train = result[0]\n",
    "        loss_train_seq, lost_train_lat = result[1], result[2]\n",
    "\n",
    "        # Calculate and save validation performance\n",
    "        batch_ind_val = np.random.choice(Nval, batch_size, replace=False)\n",
    "\n",
    "        result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.merged],\n",
    "                          feed_dict={model.x: dataTest[batch_ind_val], model.keep_prob: 1.0})\n",
    "        perf_collect[1, step] = loss_val = result[0]\n",
    "        loss_val_seq, lost_val_lat = result[1], result[2]\n",
    "        # and save to Tensorboard\n",
    "        summary_str = result[3]\n",
    "\n",
    "        print(\"At %6s / %6s train (%5.3f, %5.3f, %5.3f), val (%5.3f, %5.3f,%5.3f) in order (total, seq, lat)\" % (\n",
    "        i, max_iterations, loss_train, loss_train_seq, lost_train_lat, loss_val, loss_val_seq, lost_val_lat))\n",
    "        step += 1\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(\"logs/ecg5000.ckpt\"), step)\n",
    "\n",
    "print(\"model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_run(z_run, label, ):\n",
    "    f1, ax1 = plt.subplots(2, 1)\n",
    "\n",
    "    # First fit a PCA\n",
    "    PCA_model = TruncatedSVD(n_components=3).fit(z_run)\n",
    "    z_run_reduced = PCA_model.transform(z_run)\n",
    "    ax1[0].scatter(z_run_reduced[:, 0], z_run_reduced[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[0].set_title('PCA on z_run')\n",
    "\n",
    "    # THen fit a tSNE\n",
    "    tSNE_model = TSNE(verbose=2, perplexity=80, min_grad_norm=1E-12, n_iter=3000)\n",
    "    z_run_tsne = tSNE_model.fit_transform(z_run)\n",
    "    ax1[1].scatter(z_run_tsne[:, 0], z_run_tsne[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[1].set_title('tSNE on z_run')\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, save_path=os.path.join(\"logs/ecg5000.ckpt-0\"))\n",
    "saver.predict()\n",
    "start = 0\n",
    "label = []  # The label to save to visualize the latent space\n",
    "z_run = []\n",
    "\n",
    "while start + batch_size < Nval:\n",
    "    run_ind = range(start, start + batch_size)\n",
    "    z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: dataTest[run_ind], model.keep_prob: 1.0})\n",
    "    z_run.append(z_mu_fetch)\n",
    "    start += batch_size\n",
    "    \n",
    "z_mu_fetch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 376, 1: 227, 2: 209, 5: 153, 4: 35})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(z_mu_fetch)\n",
    "kmeans.labels_ +=1\n",
    "kmeans.labels_\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "res = kmeans.fit_predict(dataTest)+1\n",
    "\n",
    "start = 0\n",
    "i = 0\n",
    "while start< 200:\n",
    "    if res[i] == 1:\n",
    "        #print(res[i],labelsTest[i])\n",
    "        i\n",
    "    start= start+1\n",
    "    i = i+1\n",
    "\n",
    "int getPairsCount(int arr[], int n, int sum) \n",
    "{ \n",
    "    int count = 0; // Initialize result \n",
    "  \n",
    "    // Consider all possible pairs and check their sums \n",
    "    for (int i=0; i<n; i++) \n",
    "        for (int j=i+1; j<n; j++) \n",
    "            if (arr[i]+arr[j] == sum) \n",
    "                count++; \n",
    "  \n",
    "    return count; \n",
    "} \n",
    "\n",
    "#plt.plot(res.unique)\n",
    "#plt.show()\n",
    "#kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the latent space coordinates of the validation set\n",
    "start = 0\n",
    "label = []  # The label to save to visualize the latent space\n",
    "z_run = []\n",
    "\n",
    "while start + batch_size < Nval:\n",
    "    run_ind = range(start, start + batch_size)\n",
    "    z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: dataTest[run_ind], model.keep_prob: 1.0})\n",
    "    z_run.append(z_mu_fetch)\n",
    "    start += batch_size\n",
    "\n",
    "z_run = np.concatenate(z_run, axis=0)\n",
    "label = labelsTest[:start]\n",
    "\n",
    "# plot_z_run(z_run, label)\n",
    "\n",
    "z_run.shape\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import linear_assignment as la\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, normalized_mutual_info_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def calcolaPurity(labelConosciute, labels):\n",
    "    contingencymatrix = contingency_matrix(labelConosciute, labels)\n",
    "    purity = (np.sum(np.amax(contingencymatrix,axis = 0))/np.sum(contingencymatrix))\n",
    "    return purity\n",
    "\n",
    "def evaluation(X_selected, X_test, n_clusters, y):\n",
    "    \"\"\"\n",
    "    This function calculates ARI, ACC and NMI of clustering results\n",
    "    Input\n",
    "    -----\n",
    "    X_selected: {numpy array}, shape (n_samples, n_selected_features}\n",
    "            input data on the selected features\n",
    "    n_clusters: {int}\n",
    "            number of clusters\n",
    "    y: {numpy array}, shape (n_samples,)\n",
    "            true labels\n",
    "    Output\n",
    "    ------\n",
    "    nmi: {float}\n",
    "        Normalized Mutual Information\n",
    "    acc: {float}\n",
    "        Accuracy\n",
    "    \"\"\"\n",
    "    k_means = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, max_iter=300,\n",
    "                     tol=0.0001, precompute_distances=True, verbose=0,\n",
    "                     random_state=None, copy_x=True, n_jobs=1)\n",
    "\n",
    "    k_means.fit(X_selected)\n",
    "    y_predict = k_means.predict(X_test)\n",
    "\n",
    "    # calculate NMI\n",
    "    nmi = normalized_mutual_info_score(y, y_predict, average_method='arithmetic')\n",
    "\n",
    "    sil = silhouette_score(X_test, y_predict, metric=\"euclidean\")\n",
    "    db_score = davies_bouldin_score(X_test, y_predict)\n",
    "    ch_score = calinski_harabasz_score(X_test, y_predict)\n",
    "    purity = calcolaPurity(y, y_predict)\n",
    "\n",
    "return nmi, sil, db_score, ch_score, purity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
