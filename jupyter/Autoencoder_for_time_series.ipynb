{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ECG200'\n",
    "data_directory = '../data/UCRArchive_2018'\n",
    "checkpoints_directory = 'checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and divide in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s loadData ../scripts/dataUtilities.py \n",
    "def loadData(direc, dataset, perm=True, ratio_train=0.8):\n",
    "    datadir = direc + '/' + dataset + '/' + dataset\n",
    "    data_train = np.genfromtxt(datadir + '_TRAIN.tsv', delimiter='\\t')\n",
    "    data_test_val = np.genfromtxt(datadir + '_TEST.tsv', delimiter='\\t')\n",
    "    data = np.concatenate((data_train, data_test_val), axis=0)\n",
    "\n",
    "    N, D = data.shape\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    if perm:\n",
    "        ind = np.random.permutation(N)\n",
    "    else:\n",
    "        ind = range(0, N)\n",
    "    return data[ind[:ind_cut], 1:], data[ind[ind_cut:], 1:], data[ind[:ind_cut], 0], data[ind[ind_cut:], 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain, dataTest, labelsTrain, labelsTest = loadData(data_directory, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building again the dataset\n",
    "all_data = np.concatenate((dataTrain, dataTest))\n",
    "all_labels = np.concatenate((labelsTrain, labelsTest)).reshape(-1, 1)\n",
    "all_dataset = np.concatenate((all_labels, all_data), axis=1)\n",
    "\n",
    "pd.DataFrame(all_data).plot(legend=False, title=\"All data\");\n",
    "all_dataset_df = pd.DataFrame(all_dataset)\n",
    "all_dataset_df.head()\n",
    "all_dataset_df.describe()\n",
    "all_dataset_df.isnull().sum().sum()\n",
    "all_dataset_df.groupby([0]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s outlier_annihilation ../scripts/dataUtilities.py \n",
    "def outlier_annihilation(df):\n",
    "    for col in df:\n",
    "        low_threshold = df[col].quantile(0.03)\n",
    "        high_threshold = df[col].quantile(0.97)\n",
    "        df.loc[df[col] < low_threshold, col] = low_threshold\n",
    "        df.loc[df[col] > high_threshold, col] = high_threshold\n",
    "    return df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "dataTrain_df = pd.DataFrame(dataTrain)\n",
    "dataTrain_df.plot(legend=False, title=\"Pre removal\");\n",
    "dataTrain_clean = outlier_annihilation(dataTrain_df)\n",
    "pd.DataFrame(dataTrain_clean).plot(legend=False, title=\"Post removal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "dataTest_df = pd.DataFrame(dataTest)\n",
    "dataTest_df.plot(legend=False, title=\"Pre removal\");\n",
    "dataTest_clean = outlier_annihilation(dataTest_df)\n",
    "pd.DataFrame(dataTest_clean).plot(legend=False, title=\"Post removal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = dataTrain_clean.shape[0]\n",
    "Nval = dataTest_clean.shape[0]\n",
    "D = dataTrain_clean.shape[1]\n",
    "sl = D  # Time series sequence length\n",
    "print('Train set samples: %s ' % (N+1))\n",
    "print('Test set samples: %s ' % (Nval))\n",
    "print('Dimensions (columns): %s ' % (D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valuate class number and min class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labelsTrain))\n",
    "base = np.min(labelsTrain)  # Check if data is 0-based\n",
    "if base != 0:\n",
    "    labelsTrain -= base\n",
    "    labelsTest -= base\n",
    "\n",
    "print('We have %s classes, %s is the min class value' % (num_classes, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s plot_data ../scripts/plotUtilities.py \n",
    "def plot_data(data, classes, plot_row=10, save = False, name = 'tmp', adjust=True):\n",
    "    counts = dict(Counter(classes))\n",
    "    uniqueClasses = np.unique(classes)\n",
    "    num_classes = len(uniqueClasses)\n",
    "    f, axarr = plt.subplots(plot_row, num_classes)\n",
    "    for selectedClass in uniqueClasses:  # Loops over classes, plot as columns\n",
    "        selectedClass = int(selectedClass)\n",
    "        ind = np.where(classes == selectedClass)\n",
    "        ind_plot = np.random.choice(ind[0], size=plot_row)\n",
    "        for n in range(plot_row):  # Loops over rows\n",
    "            # Only shops axes for bottom row and left column\n",
    "            if n == 0:\n",
    "                axarr[n, selectedClass].set_title('Class %.0f (%.0f elements)' % (selectedClass + 1, counts[float(selectedClass)]))\n",
    "            if n < counts[float(selectedClass)]:\n",
    "                axarr[n, selectedClass].plot(data[ind_plot[n], :])\n",
    "\n",
    "                if not n == plot_row - 1:\n",
    "                    plt.setp([axarr[n, selectedClass].get_xticklabels()], visible=False)\n",
    "                if not selectedClass == 0:\n",
    "                    plt.setp([axarr[n, selectedClass].get_yticklabels()], visible=False)\n",
    "    \n",
    "    if adjust == True:\n",
    "        f.subplots_adjust(hspace=0)  # No horizontal space between subplots\n",
    "        f.subplots_adjust(wspace=0)  # No vertical space between subplots\n",
    "    plt.show()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(name, format='png', dpi=1000)\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data class plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "plot_data(dataTrain, labelsTrain)\n",
    "# plot_data(dataTrain, labelsTrain, 5, True, \"Prova.eps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 100  # after _plot_every_ GD steps, there's console output\n",
    "max_iterations = 1000  # maximum number of iterations\n",
    "dropout = 0.8 # Dropout rate\n",
    "\n",
    "dataset_configs = {\n",
    "    'ECG5000': {\n",
    "        'num_layers': 2,  # number of layers of stacked RNN's\n",
    "        'hidden_size': 90,  # memory cells in a layer\n",
    "        'max_grad_norm': 5,  # maximum gradient norm during training\n",
    "        'batch_size': 64, # number of samples for iteration\n",
    "        'learning_rate': .005, # for exponential decay\n",
    "        'crd': 1,  # Hyperparameter for future generalization\n",
    "        'num_l': 32 # number of units in the latent space\n",
    "    },\n",
    "    'ECG200': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    },\n",
    "    'ChlorineConcentration': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    },\n",
    "    'FordA': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    },\n",
    "    'FordB': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    },\n",
    "    'PhalangesOutlinesCorrect': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    },\n",
    "    'RefrigerationDevices': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    },\n",
    "    'TwoLeadECG': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': .001,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    },\n",
    "    'TwoPatterns': {\n",
    "        'num_layers': 2,\n",
    "        'hidden_size': 90,\n",
    "        'max_grad_norm': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': .005,\n",
    "        'crd': 1,\n",
    "        'num_l': 32\n",
    "    }\n",
    "}\n",
    "config = dataset_configs[dataset_name]\n",
    "config['sl'] = sl;  # Time series sequence length\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net description\n",
    "**dropout**: A single model can be used to simulate having a large number of different network architectures by randomly dropping out nodes during training. This is called dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and improve generalization error in deep neural networks of all kinds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.nn.rnn_cell import LSTMCell\n",
    "import tensorflow_probability as tfp\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, config):\n",
    "        # Hyperparameters of the net\n",
    "        num_layers = config['num_layers']\n",
    "        hidden_size = config['hidden_size']\n",
    "        max_grad_norm = config['max_grad_norm']\n",
    "        batch_size = config['batch_size']\n",
    "        sl = config['sl']\n",
    "        crd = config['crd']\n",
    "        num_l = config['num_l']\n",
    "        learning_rate = config['learning_rate']\n",
    "        self.sl = sl\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Nodes for the input variables\n",
    "        self.x = tf.placeholder(\"float\", shape=[batch_size, sl], name='Input_data')\n",
    "        self.x_exp = tf.expand_dims(self.x, 1)\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            # Th encoder cell, multi-layered with dropout\n",
    "            # Number of LSTM = hidden layer size\n",
    "            cell_enc = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "            cell_enc = tf.contrib.rnn.DropoutWrapper(cell_enc, output_keep_prob=self.keep_prob)\n",
    "\n",
    "            # Initial state, tuple for all lstms stacked\n",
    "            initial_state_enc = cell_enc.zero_state(batch_size, tf.float32)\n",
    "\n",
    "            # layer for mean of z\n",
    "            W_mu = tf.get_variable('W_mu', [hidden_size, num_l])\n",
    "\n",
    "            # Creates a recurrent neural network specified by RNNCell cell\n",
    "            # outputs is a length T list of outputs (one for each input), or a nested tuple of such elements.\n",
    "            # in our case one output for each time series in input\n",
    "            outputs_enc, _ = tf.contrib.rnn.static_rnn(cell_enc,\n",
    "                                                       inputs=tf.unstack(self.x_exp, axis=2),\n",
    "                                                       initial_state=initial_state_enc)\n",
    "            cell_output = outputs_enc[-1]\n",
    "            b_mu = tf.get_variable('b_mu', [num_l])\n",
    "\n",
    "            # self.z_mu is the Tensor containing the hidden representations\n",
    "            # It can be used to do visualization, clustering or subsequent classification\n",
    "            # tf.nn.xw_plus_b computes matmul(x, weights) + biases.\n",
    "            self.z_mu = tf.nn.xw_plus_b(cell_output, W_mu, b_mu, name='z_mu')  # mu, mean, of latent space\n",
    "\n",
    "            # Calculate the mean and variance of the latent space\n",
    "            # The mean and variance are calculated by aggregating the contents of z_mu across axes\n",
    "            lat_mean, lat_var = tf.nn.moments(self.z_mu, axes=[1])\n",
    "\n",
    "            # Train the point in latent space to have zero-mean and unit-variance on batch basis\n",
    "            self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean) + lat_var - tf.log(lat_var) - 1)\n",
    "\n",
    "        with tf.name_scope(\"Lat_2_dec\"):\n",
    "            # layer to generate initial state\n",
    "            W_state = tf.get_variable('W_state', [num_l, hidden_size])\n",
    "            b_state = tf.get_variable('b_state', [hidden_size])\n",
    "            z_state = tf.nn.xw_plus_b(self.z_mu, W_state, b_state, name='z_state')  # mu, mean, of latent space\n",
    "\n",
    "        # Similar steps as encoder\n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            # The decoder, also multi-layered\n",
    "            cell_dec = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_dec = tuple([(z_state, z_state)] * num_layers)\n",
    "            dec_inputs = [tf.zeros([batch_size, 1])] * sl\n",
    "\n",
    "            outputs_dec, _ = tf.contrib.rnn.static_rnn(cell_dec,\n",
    "                                                       inputs=dec_inputs,\n",
    "                                                       initial_state=initial_state_dec)\n",
    "        with tf.name_scope(\"Out_layer\"):\n",
    "            params_o = 2 * crd  # Number of coordinates + variances\n",
    "            W_o = tf.get_variable('W_o', [hidden_size, params_o])\n",
    "            b_o = tf.get_variable('b_o', [params_o])\n",
    "            outputs = tf.concat(outputs_dec, axis=0)  # tensor in [sl*batch_size,hidden_size]\n",
    "            h_out = tf.nn.xw_plus_b(outputs, W_o, b_o)\n",
    "            h_mu, h_sigma_log = tf.unstack(tf.reshape(h_out, [sl, batch_size, params_o]), axis=2)\n",
    "            h_sigma = tf.exp(h_sigma_log)\n",
    "            dist = tfp.distributions.Normal(h_mu, h_sigma)\n",
    "            px = dist.log_prob(tf.transpose(self.x))\n",
    "            loss_seq = -px\n",
    "            self.loss_seq = tf.reduce_mean(loss_seq)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            # Use learning rate decay\n",
    "            # Useful use a learning rate schedule to reduce learning rate as the training progresses. \n",
    "            lr = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.1, staircase=False)\n",
    "\n",
    "            self.loss = self.loss_seq + self.loss_lat_batch\n",
    "\n",
    "            # Route the gradients\n",
    "            tvars = tf.trainable_variables()\n",
    "            # We clip the gradients to prevent explosion\n",
    "            grads = tf.gradients(self.loss, tvars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "            self.numel = tf.constant([[0]])\n",
    "\n",
    "            # And apply the gradients\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "            gradients = zip(grads, tvars)\n",
    "            self.train_step = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "\n",
    "            self.numel = tf.constant([[0]])\n",
    "            \n",
    "        tf.summary.tensor_summary('lat_state', self.z_mu)\n",
    "        # Define one op to call all summaries\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # and one op to initialize the variables\n",
    "        self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config)\n",
    "sess = tf.Session()\n",
    "perf_collect = np.zeros((2, int(np.ceil(max_iterations / plot_every)+1)))\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "# Needed later for testing K-means\n",
    "actual_test_size = (Nval // batch_size) * batch_size\n",
    "actual_dataTest = dataTest_clean[0:actual_test_size]\n",
    "actual_labelsTest = labelsTest[0:actual_test_size]\n",
    "\n",
    "# Start of the train\n",
    "epochs = np.floor(batch_size * max_iterations / N)\n",
    "\n",
    "print('Train with approximately %d epochs' % epochs)\n",
    "\n",
    "sess.run(model.init_op)\n",
    "\n",
    "step = 0  # Step is a counter for filling the numpy array perf_collect\n",
    "for i in range(max_iterations):\n",
    "    batch_ind = np.random.choice(N, batch_size, replace=False)\n",
    "    result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n",
    "                      feed_dict={model.x: dataTrain_clean[batch_ind], model.keep_prob: dropout})\n",
    "    \n",
    "    if (i == 0) or (((i+1) % plot_every) == 0):\n",
    "        # Save train performances\n",
    "        perf_collect[0, step] = loss_train = result[0]\n",
    "        loss_train_seq, lost_train_lat = result[1], result[2]\n",
    "\n",
    "        # Calculate and save validation performance\n",
    "        batch_ind_val = np.random.choice(Nval, batch_size, replace=False)\n",
    "\n",
    "        result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.merged],\n",
    "                          feed_dict={model.x: dataTest_clean[batch_ind_val], model.keep_prob: 1.0})\n",
    "        perf_collect[1, step] = loss_val = result[0]\n",
    "        loss_val_seq, lost_val_lat = result[1], result[2]\n",
    "        # and save to Tensorboard\n",
    "        summary_str = result[3]\n",
    "\n",
    "        print(\"At %6s / %6s train (%5.3f, %5.3f, %5.3f), val (%5.3f, %5.3f,%5.3f) in order (total, seq, lat)\" % (\n",
    "        i+1, max_iterations, loss_train, loss_train_seq, lost_train_lat, loss_val, loss_val_seq, lost_val_lat))\n",
    "        step += 1\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(checkpoints_directory, dataset_name))\n",
    "\n",
    "print(\"model trained, saved in logs directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get latent vector from model run on dataTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, save_path=os.path.join(checkpoints_directory, dataset_name))\n",
    "\n",
    "# Extract the latent space coordinates of the validation set\n",
    "\n",
    "start = 0\n",
    "label = []  # The label to save to visualize the latent space\n",
    "z_run_clust = [] # Latent space for clustering\n",
    "z_run_emb = [] # Latent space for tSNE\n",
    "\n",
    "while start + batch_size < Nval:\n",
    "    run_ind = range(start, start + batch_size)\n",
    "    z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: actual_dataTest[run_ind], model.keep_prob: 1.0})\n",
    "    z_run_clust.extend(z_mu_fetch.tolist())\n",
    "    z_run_emb.append(z_mu_fetch)\n",
    "\n",
    "    start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize latent vector on PCA and tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_z_run(z_run_emb, label, ):\n",
    "    f1, ax1 = plt.subplots(2, 1)\n",
    "\n",
    "    # First fit a PCA\n",
    "    PCA_model = TruncatedSVD(n_components=3).fit(z_run_emb)\n",
    "    z_run_reduced = PCA_model.transform(z_run_emb)\n",
    "    ax1[0].scatter(z_run_reduced[:, 0], z_run_reduced[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[0].set_title('PCA on z_run')\n",
    "\n",
    "    # Then fit a tSNE\n",
    "    tSNE_model = TSNE(verbose=2, perplexity=80, min_grad_norm=1E-12, n_iter=3000)\n",
    "    z_run_tsne = tSNE_model.fit_transform(z_run_emb)\n",
    "    ax1[1].scatter(z_run_tsne[:, 0], z_run_tsne[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[1].set_title('tSNE on z_run')\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_run_emb = np.concatenate(z_run_emb, axis=0)\n",
    "label = labelsTest[:start]\n",
    "plot_z_run(z_run_emb, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot entire test set on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "plot_data(actual_dataTest, actual_labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based on latent vector of net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClustering(features, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=100)\n",
    "    clustering = kmeans.fit_predict(features)\n",
    "    kmeans.labels_ += 1\n",
    "    clustering = clustering -1\n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = num_classes\n",
    "clustering = getClustering(z_run_clust, num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test set on clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (16, 8)\n",
    "plot_data(actual_dataTest, clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score, adjusted_mutual_info_score, homogeneity_score, completeness_score, v_measure_score, fowlkes_mallows_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_labels =[ int(lt) + 1 for lt in actual_labelsTest ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dunn Index is heavy and good for small datasets. Not used\n",
    "\n",
    "# Misura la densità del clustering, ovvero quanto un sample è simile agli altri punto dello stesso cluster \n",
    "# e quanto bene dista dal cluster più vicino usando una metrica di similarità (euclidea, cosine, ecc).\n",
    "# Questo score è la media di tutti i silhouette score di ciascun sample\n",
    "# DTW is fine for TS but it takes too long\n",
    "ss = silhouette_score(actual_dataTest, clustering, metric='euclidean')\n",
    "print(\"Silhouette score\", ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davies Bouldin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DB: Misura la separazione tra cluster, compiendo una media artimetica delle similarità tra coppie di cluster più simili\n",
    "# , usandodi una misura di similarità tra cluster ad hoc che mette a rapporto la somma dei diametri \n",
    "# dei cluster (media distanza euclidea intra-cluster) e la distanza euclidea tra i rispettivi centroidi.\n",
    "# Più tende a 0 meglio è. Fvorisce cluster densi e ben distanti \n",
    "# Più veloce di silhouette ma limitato alla distanza euclidea\n",
    "db = davies_bouldin_score(actual_dataTest, clustering)\n",
    "print(\"Davies Bouldin\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Righe le label e colonne i cluster\n",
    "cm = contingency_matrix(ground_labels, clustering)\n",
    "print(\"Contingency matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media tra tutti i cluster del numero di sample della label più presente di ciascun cluster.\n",
    "# Da' una misura di quanto bene il clustering copre il labelling. Se è 1, il clustering ha coperto tutte le label\n",
    "# , anche ricorrendo ad un numero di cluster maggiore delle classi\n",
    "def purity(y_true, y_pred):\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(cm, axis=0)) / np.sum(cm) \n",
    "\n",
    "pur = purity(ground_labels, clustering)\n",
    "print(\"Purity: \", pur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_purity(y_true, y_pred):\n",
    "    cm = contingency_matrix(y_true, y_pred)\n",
    "    labels_sum = np.sum(cm, axis=1)\n",
    "    rm = np.zeros(cm.shape)\n",
    "    for j in range(cm.shape[1]):\n",
    "        for i in range(cm.shape[0]):\n",
    "            rm[i][j] = cm[i][j] / labels_sum[i]\n",
    "    #print(\"Relative Contingency Matrix\")\n",
    "    #print(rm)\n",
    "    #print(np.max(rm, axis=0))\n",
    "    \n",
    "    max_indexes = np.argmax(rm, axis=0)\n",
    "    #print(max_indexes)\n",
    "    sum = 0\n",
    "    for j in range(rm.shape[1]):\n",
    "        sum += cm[max_indexes[j]][j]\n",
    "    return sum / np.sum(cm)\n",
    "\n",
    "rel_pur = rel_purity(ground_labels, clustering)\n",
    "print(\"Relative Purity: \", rel_pur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted rand index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARI: fix dell'RI, che mette a rapporto il numero di true (se due sample sono nello stesso cluster allora hanno la stessa label \n",
    "# + se due sample sono in cluster diversi allora hanno diversa label) sul numero totaale di coppie non ordinate di sample\n",
    "# va bene quando si vuole un clustering molto fedele al labelling del dataset. Valida per dataset i cui sample appartengono a classi ben distanti.\n",
    "# Immune al random labelling: https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py\n",
    "# Rule of thumb: Use ARI when the ground truth clustering has large equal sized clusters\n",
    "ars = adjusted_rand_score(ground_labels, clustering)\n",
    "print(\"Adjusted rand index: \", ars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fowlkes-Mallows score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMS: Media geometrica di precision e recall pairwise\n",
    "fms = fowlkes_mallows_score(ground_labels, clustering)\n",
    "print(\"Fowlkes-Mallows score: \", fms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMIS: fix del MIS, basata sull'entropia di Von Neuman, calcolata per le label e per i cluster\n",
    "# Immune al random labelling\n",
    "# Rule of thumb: Usa AMI when the ground truth clustering is unbalanced and there exist small clusters\n",
    "amis = adjusted_mutual_info_score(ground_labels, clustering, average_method='arithmetic')\n",
    "print(\"Adjusted mutual information: \", amis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media armonica di Homogeneity e Completeness. \n",
    "# Homogeneity: Quanto un cluster ha sample di una sola label\n",
    "# Completeness: Quanto i sample di una label stanno in un solo cluster\n",
    "# Entrambi basati sull'entropia di Von Neumann\n",
    "# Debole al random clustering con alto numero di cluster. Non buono con dataset piccoli e/o grande numero di cluster\n",
    "vm = v_measure_score(ground_labels, clustering)\n",
    "print(\"V-Measure: \", vm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
