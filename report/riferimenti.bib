@article{tsfresh,
	title = {Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package)},
	journal = {Neurocomputing},
	volume = {307},
	pages = {72 - 77},
	year = {2018},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2018.03.067},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231218304843},
	author = {Maximilian Christ and Nils Braun and Julius Neuffer and Andreas W. Kempa-Liehr},
	keywords = {Feature engineering, Time series, Feature extraction, Feature selection, Machine learning},
	abstract = {Time series feature engineering is a time-consuming process because scientists and engineers have to consider the multifarious algorithms of signal processing and time series analysis for identifying and extracting meaningful features from time series. The Python package tsfresh (Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests) accelerates this process by combining 63 time series characterization methods, which by default compute a total of 794 time series features, with feature selection on basis automatically configured hypothesis tests. By identifying statistically significant time series characteristics in an early stage of the data science process, tsfresh closes feedback loops with domain experts and fosters the development of domain specific features early on. The package implements standard APIs of time series and machine learning libraries (e.g. pandas and scikit-learn) and is designed for both exploratory analyses as well as straightforward integration into operational data science applications.}
}

@misc{tslearn,
	title={tslearn: A machine learning toolkit dedicated to time-series data},
	author={Tavenard, Romain},
	year={2017},
	note={\url{https://github.com/rtavenar/tslearn}}
}

@book{sklearn_api,
	author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
	Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
	Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
	and Jaques Grobler and Robert Layton and Jake VanderPlas and
	Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
	title = {{API} design for machine learning software: experiences from the scikit-learn
	project},
	booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
	year = {2013},
	pages = {108--122},
}

@misc{numpy,
	author =    {Travis Oliphant},
	title =     {{NumPy}: A guide to {NumPy}},
	year =      {2006--},
	howpublished = {USA: Trelgol Publishing},
	url = {http://www.numpy.org/},
	note = {[Online; accessed <today>]}
}

@article{matplotlib,
	Author    = {Hunter, J. D.},
	Title     = {Matplotlib: A 2D graphics environment},
	Journal   = {Computing in Science \& Engineering},
	Volume    = {9},
	Number    = {3},
	Pages     = {90--95},
	abstract  = {Matplotlib is a 2D graphics package used for Python for
	application development, interactive scripting, and publication-quality
	image generation across user interfaces and operating systems.},
	publisher = {IEEE COMPUTER SOC},
	doi       = {10.1109/MCSE.2007.55},
	year      = 2007
}

@book{pandas,
	author    = { Wes McKinney },
	title     = { Data Structures for Statistical Computing in Python},
	booktitle = { Proceedings of the 9th Python in Science Conference},
	pages     = { 51 - 56 },
	year      = { 2010 },
	editor    = { St\'efan van der Walt and Jarrod Millman }
}

@misc{metrics,
	title = {Clustering - Scikit Learn},
	howpublished = {\url{https://scikit-learn.org/stable/modules/clustering.html}},
	note = {Accessed: 2019-07-23}
}

@article{ari,
	author = {Yeung, Ka Yee and Ruzzo, Walter},
	year = {2001},
	month = {01},
	pages = {},
	title = {Details of the Adjusted Rand index and Clustering algorithms Supplement to the paper "An empirical study on Principal Component Analysis for clustering gene expression data" (to appear in Bioinformatics)},
	volume = {17},
	journal = {Science}
}

@ARTICLE{long_term, 
	author={Y. {Bengio} and P. {Simard} and P. {Frasconi}}, 
	journal={IEEE Transactions on Neural Networks}, 
	title={Learning long-term dependencies with gradient descent is difficult}, 
	year={1994}, 
	volume={5}, 
	number={2}, 
	pages={157-166}, 
	keywords={recurrent neural nets;learning (artificial intelligence);numerical analysis;long-term dependencies;gradient descent;recognition;production problems;prediction problems;recurrent neural network training;temporal contingencies;input/output sequence mapping;efficient learning;Recurrent neural networks;Production;Delay effects;Intelligent networks;Neural networks;Discrete transforms;Computer networks;Cost function;Neurofeedback;Displays}, 
	doi={10.1109/72.279181}, 
	ISSN={1045-9227}, 
	month={March},
}

@article{lstm,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	month = {12},
	pages = {1735-80},
	title = {Long Short-term Memory},
	volume = {9},
	journal = {Neural computation},
	doi = {10.1162/neco.1997.9.8.1735}
}

@article{dtw, 
	author={H. {Sakoe} and S. {Chiba}}, 
	journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
	title={Dynamic programming algorithm optimization for spoken word recognition}, 
	year={1978}, 
	volume={26}, 
	number={1}, 
	pages={43-49}, 
	keywords={Dynamic programming;Heuristic algorithms;Fluctuations;Timing;Signal processing algorithms;Speech processing;Pattern matching;Constraint optimization;Feature extraction;Acoustics}, 
	doi={10.1109/TASSP.1978.1163055}, 
	ISSN={0096-3518}, 
	month={February},
}